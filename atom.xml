<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>artanis home</title>
  
  <subtitle>生活像是一场游戏，有人陪伴才最好</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-07-11T08:24:41.880Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Artanis</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>imageNet 小结</title>
    <link href="http://yoursite.com/2018/07/09/imageNet-%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2018/07/09/imageNet-小结/</id>
    <published>2018-07-09T09:24:16.000Z</published>
    <updated>2018-07-11T08:24:41.880Z</updated>
    
    <content type="html"><![CDATA[<p>久违的更新～～～，时值暑假，正好把之前没看过的paper仔细的看一遍，也同时为了给自己打下一个更好的基础。我就按照github上的paper的分类精选一个个看下来，paper较多，大约有一百多篇，一个暑假可能看不完，但是我会尽量多看些，并将博客的这个paper系列继续下去。今天讨论的主要是imageNet的演变过程。</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>AlexNet算是deep learning的重大突破，由于它，deep learning开始被广泛使用。</p><p>开始，我们需要知道ImageNet是什么，ImageNet是一个超过1500万已标注的高精度图片，包含有22000个种类(当时的数据)，从2010年开始，举办方举办了一个年度的竞赛叫ILSVRC(ImageNet Large-Scale Visual Recognition Challenge)。其中ILSVRC2010的测试数据集是可以利用的，因此，本paper的大部分试验都在这个数据集上做的。</p><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>文章首先提出，现实中的问题不像ImageNet上的数据集一样如此特定和单一，因此，对于现实中的问题，需要的模型应该有足够的复杂包容度。而卷积神经网络恰恰有足够的容量（learning capacity）和容量控制能力（随意的控制深度和宽度）。</p><p>由于ImageNet包含不同精度的图片，而我们的神经网络要求固定的输入维度。因此图片采用下采样到256×256。下采样的方法是先rescale将宽变为256，再在中心裁剪出256×256的图像。</p><p>激活函数方面paper使用的是RELU，对比于tanh以及sigmoid这些linear activations. RELU这样的non-linear activations 能够生成non-saturating neurons（处理之后没有被挤压到一个特定区间的值），在训练梯度下降的时间方面，non-linear activation更快。</p><p>在使用GPU方面，由于当时GPU的限制，论文中使用了两个GPU，现在自然不再需要。</p><p>normalization方面，采用Local Response Normalization，增强泛化。</p><p>pooling方面，采用overlap pooling，也就是sizeX &gt; stride从而导致相邻池化窗口有重叠。</p><p>减少过拟合方面，采用了两种方法。一种是通过数据增强来人为的增加数据量。一种是dropout。数据增强是将原有的图片提取随机的224 * 224的部分(patches)以及它的水平投影(horizontal reflection), 取四个角落以及中间的部分以及它们的水平投影也就是有10个部分(patches)。还有一种数据增强则是改变训练图片RGB channels的强度。特别的，使用PCA(Principal Component Analysis)来处理ImageNet训练集中一系列的RGB元素值。PCA主要通过将原始数据变换为一组各维度线性无关的表示，可用于提取数据主要特征分量，常用于高维数据的降维。有关PCA的数学原理见<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">PCA的数学原理</a>，<br>第二种方法是使用Dropout的时候，采用随机dropout。</p><p>训练的细节：</p><ul><li>batchsize 128</li><li>momentum 0.9</li><li>weight decay 0.0005</li></ul><p>对于权重w的更新规则如下图所示：<br><img src="/images/imageNet2.png" alt=""></p><p>其中v是momentum的变量，i是循环的下标，还有一个不会读的是学习速率。</p><p>初始化权重的时候使用一个以0为平均值的高斯分布，其中标准偏差为0.01，对第二层第四层第五层的卷积层以及全连接层的神经元初始化偏置为1。剩下的偏置为0。</p><p>训练的速率对于所有的层都是一样。调整采用的是启发法，初始的速率设置为0.01，每当validation error不再下降的时候，速率就除以10，直到执行这个操作3次以后为止。</p><p>总的架构下图所示。<br><img src="/images/imageNet1.png" alt=""></p><h2 id="VggNet"><a href="#VggNet" class="headerlink" title="VggNet"></a>VggNet</h2><p>vggNet总结了在AlexNet出现之后近些年的一些成果，是其集大成者，一方面的改进是使用更小的窗口(receptive window)和更小的步长(stride)。另外一个方面的改进是训练和测试网络分开,训练采用整张图片以及多尺度的图片。而本片论文提高方向则是增加深度，由于使用的比较小的窗口(3 × 3)，参数量较小，因此增加深度是可行的。</p><h3 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a>architecture</h3><p>输入: 224 × 224<br>预处理 : subtract the mean of the RGB value (training set, each pixel)</p><p>总体架构如下图所示：<br><img src="/images/imageNet3.png" alt=""></p><h4 id="滤波器"><a href="#滤波器" class="headerlink" title="滤波器"></a>滤波器</h4><p>使用三个3 × 3的小窗口来代替1一个7 × 7的窗口的原因：</p><ul><li>有三个non-linear rectifiction layer而不是一个可以让决策函数更具有辨别能力(make the decision function more discriminative)</li><li>参数量：假设有C个channel，则前一个的参数量为3 × （3 × 3 × C），后一个的参数量则为7 × 7 × C。</li></ul><p>1 × 1 卷积层的加入</p><ul><li><p>增加decision function的非线性性，同时也能够避免影响卷积层的感受野。尽管卷积层本身是线性映射，但是由于还增加了RELU层，所以引进了非线性性。</p></li><li><p>专注于跨通道的特征组合，不考虑局部信息组合。</p></li><li><p>对feature map的channel级别降维或者升维。</p><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>一般常见的网络的结构大多为INPUT -&gt; [[CONV -&gt; RELU]×N -&gt; POOL?]×M -&gt; [FC -&gt; RELU]×K -&gt; FC，vgg net的池化层不同于Alex Net采用的3 × 3 stride为2的maxpooling，而是采用的2 × 2 stride为2的maxpooling。池化层做的事情是根据对应的max或average的方式进行特征筛选。小的kernel带来的是更细节的信息捕获。</p></li></ul><h4 id="特征图"><a href="#特征图" class="headerlink" title="特征图"></a>特征图</h4><p>网络在随层数递增的过程中，通过池化也逐渐忽略局部信息，池化操作使得宽高每一次都变为变为一半，而深度每次增加一倍。信息分散到channel层级上。特征图从channel从512开始进入全连接层，全连接层与卷积层相比更考虑全局信息，三个全连接是为了捕捉特征映射来去之间的细微变化。</p><h4 id="layer-pattern"><a href="#layer-pattern" class="headerlink" title="layer pattern"></a>layer pattern</h4><p>vgg net 有两种layer pattern，第二种（[conv-relu]-[conv-relu]-[conv-relu]-pool）比第一种（[conv-relu]-[conv-relu]-pool） 多了一个[conv-relu]，我的理解是</p><ul><li>开始时feature map的local特征信息更加多一些，之后通过增加conv filter的方式扩大channel的数目，在一个layer pattern的中层，channel数已经足够，当再想提取lacal的信息的时候，就加一层[conv - relu]的形式来压榨提炼特征。类似于使用非线性变换对已有特征进行变换，从而产生更多的特征。</li><li>多出的conv对网络中层进一步进行学习和控制保证特征信息不漂移到channel级别上。</li><li>conv本身更加注重单张feature map的局部信息，尽量去平衡channel(depth)级别与local级别(width, height)，控制特征信息不要过于向channel级别偏移。</li></ul><p>关于layer pattern</p><ul><li>串联和串联中带有并联的网络架构。GoogleNet引入inception模块，ResNet引入Residual Block。串联带有一些并联同类操作但不同参数的模块在特征提取上更好(做特征工程)</li><li>用在ImageNet上预训练的模型。</li></ul><h3 id="training-detail"><a href="#training-detail" class="headerlink" title="training detail"></a>training detail</h3><p>在训练的细节方面，几乎与AlexNet相同，额外的bathsize选择的是256。在初始化的方面，需要非常小心，首先对于上图中的configuration A,采用随机初始化进行训练，然后对于更深的模型，将前四个卷积层以及最后三层的全连接层使用A的参数，中间的层采用随机初始化。</p><p>全连接层的随机初始化：</p><ul><li>FC4096-ReLU6-Drop0.5，FC为高斯分布初始化（std=0.005），bias常数初始化（0.1）</li><li>FC4096-ReLU7-Drop0.5，FC为高斯分布初始化（std=0.005），bias常数初始化（0.1）</li><li>FC1000（最后接SoftMax1000分类），FC为高斯分布初始化（std=0.005），bias常数初始化（0.1）<br>实际并非采用的Alex net的std = 0.1， bias = 0。</li></ul><h3 id="test-detail"><a href="#test-detail" class="headerlink" title="test detail"></a>test detail</h3><p>参考overfeat，测试的时候将最后的全连接层该为卷积层，从而可以处理任意分辨率的图片，无需对原图进行resize。但是若原图大于resize之后的图片，那么将最后的s × s的feature map进行average，最终交给softmax的还是1 × 1的feature map。<br>在介绍下面的模块之前首先我们需要先了解一下FCN(全卷积网络)</p><p>测试的细节方面</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;久违的更新～～～，时值暑假，正好把之前没看过的paper仔细的看一遍，也同时为了给自己打下一个更好的基础。我就按照github上的paper的分类精选一个个看下来，paper较多，大约有一百多篇，一个暑假可能看不完，但是我会尽量多看些，并将博客的这个paper系列继续下去。
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>近日学习计划——4.21</title>
    <link href="http://yoursite.com/2018/04/21/%E8%BF%91%E6%97%A5%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92%E2%80%94%E2%80%944-21/"/>
    <id>http://yoursite.com/2018/04/21/近日学习计划——4-21/</id>
    <published>2018-04-21T15:29:49.000Z</published>
    <updated>2018-04-21T15:40:19.641Z</updated>
    
    <content type="html"><![CDATA[<h4 id="deadline-5-1"><a href="#deadline-5-1" class="headerlink" title="deadline : 5.1"></a>deadline : 5.1</h4><ul><li><p>Deep Learning: Linear algebra chapter and Probability and Information Theory chapter</p></li><li><p>CTPN paper</p></li><li><p>AMSGrad Paper</p></li><li><p>self-attention Paper</p></li><li><p>CTPN(Chinese OCR) source code</p></li><li><p>attention source code</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;deadline-5-1&quot;&gt;&lt;a href=&quot;#deadline-5-1&quot; class=&quot;headerlink&quot; title=&quot;deadline : 5.1&quot;&gt;&lt;/a&gt;deadline : 5.1&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Deep Learning: Li
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Batchnormalization 论文解读</title>
    <link href="http://yoursite.com/2018/04/12/Batchnormalization-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <id>http://yoursite.com/2018/04/12/Batchnormalization-论文解读/</id>
    <published>2018-04-12T15:20:56.000Z</published>
    <updated>2018-04-14T13:54:03.147Z</updated>
    
    <content type="html"><![CDATA[<p>训练深度神经网络的时候经常会出现的问题是在每次训练的时候输入的数据发生一丝微小的改变，会改变之后的所有层，并且层数越深影响越大。也可以说当输入的分布发生变化，网络就不得不再去适应新的数据分布。这会导致一个模型非常难训练出来。一旦训练速率与参数初始化不恰当，模型就不会收敛。这个现象被称之为covariate shift。并且，对于每一个网络层输入的变化都会产生covariate shift 的现象，作者把内部的网络节点输入的变化称之为Internal Covariate Shift. 第二个问题是梯度弥散或者梯度爆炸的问题，当网络深度很深的时候，前面参数的细微变化会导致后面参数更新变化非常大。</p><p>解决这个问题通常使用normalizing layer——输入白化（转化为平均值和方差为0，并且去相关花）, 并且可以考虑在每个training step以及某一些间隔插入normalizing layer。但是根据论文的举例，使用normalizing layer也就是白化会导致某些参数的更新的效果被抵消，这样的话更新会一直进行下去，但是输出与损失函数并没有发生变化。之所以更新会抵消是因为梯度下降忽略了normalizing layer 参数对其他参数的依赖。也就是梯度流忽略了normalizing layer。如果求梯度的时候考虑normalizing layer，则计算非常的耗时，需要计算协方差矩阵以及白化部分。并且白化并不是处处可微。</p><p>有两个方法简化计算，一个是讲每个特征当做是独立随机变量单独进行规范化，这样就不存在协方差矩阵。另一个是在每个mini-batch中计算mini-batch和variance来代替整个数据集的mini-batch和variance.因为对于mini-batch gradient decent，计算整个数据集的mini-batch和variance是不切实际的。</p><p>如果仅仅将每一层的输入都进行规范化，那么会改变原先这一层的表达能力，比如本来有两个值，一个在sigmoid函数的线性区域，一个在sigmoid函数的非线性区域，进行规范化后两个值就都作用在了sigmoid函数的线性区域了，原来sigmoid函数的表达能力就没有了。所以加上了一对参数γ和β，来scale和shift规范化的值。</p><p>为什么Batch Normalization会解决数据分布的问题以及梯度弥散的问题? 首先看数据分布的问题，一般数据z之间都是互相关的，拿二维的数据进行举例，假设第一次输入的数据都是主要都是在第一象限。</p><p><img src="/images/1.jpg" alt=""></p><p>在之后经过每一层的操作之后，数据分布都会发生变化，如果网络足够深的话，后面几层的网络接受的输入分布会与之前的数据分布完全不一样。我们希望数据的分布最好是稳定的。所以我应用Batch normalization。使得每次输出的数据都是在四个象限比较均匀的分布。这样数据就会相对的稳定。</p><p>再看解决梯度弥散或者梯度爆炸的问题, 前向传播的时候有：</p><p><img src="/images/Picture20.png" alt=""></p><p>反向传播的时候有：</p><p><img src="/images/Picture21.png" alt=""></p><p>从l层传播到k层则有:</p><p><img src="/images/Picture22.png" alt=""></p><p>如果w大多小于1, 则会产生极小的梯度，如果w大多大于1则会相反。所以使用Batch normalization来解决这个问题。</p><p>首先：</p><p><img src="/images/Picture23.png" alt=""></p><p>反向传导的时候便有：</p><p><img src="/images/2.jpg" alt=""></p><p>第一个式子指的是：对于无论w多大，对梯度流都有影响。第二个式子指的是，w越大，产生的更新越小，有了这两个特点的保证，就可以保证w的更新更加稳定。</p><p>对其本质的思考，训练其实是训练模型的表达能力，区分不同的数据分布，所以，模型会越来越深，表达能力越来越好，但是，模型过深，会产生许多问题，比如数据分布的细微变化，参数细微的变化就会导致后续巨大的变化，导致过拟合，模型的泛化能力很弱，所以，对许多层的数据加入归一化，使得数据分布变化不会那么大，后续模型参数变化也不会过大，但是这样模型的表达能力就变弱了。所以加入γ与β，通过训练γ与β，达到表达能力与泛化能力之间的很好的平衡。</p><p>下面是一些检测自己是否理解了Batch Normalization的问题，有兴趣的读者可以看了回忆一下。</p><p>问题一：Batchnormalization中说的把每个特征单独做规范化中的每个特征在CNN中是指的什么，batchsize中的m在CNN中指的是什么？</p><p>问题二：CNN中做Batchnormalization z = g(BN(Wu))</p><p>问题三：为什么要加γ和β的偏置。</p><p><img src="/images/3.jpg" alt=""></p><p>问题四：白化的偏导为什么计算量比较大并且某些地方不可微。</p><p>问题五：Batchnormalization 放在哪边</p><p>问题六：internal covariate shift</p><p>问题七：test的时候均值与方差从哪来的。</p><p>问题八：如何解决梯度爆炸或梯度消失的。</p><p>问题九：如何解决数据分布的变化。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;训练深度神经网络的时候经常会出现的问题是在每次训练的时候输入的数据发生一丝微小的改变，会改变之后的所有层，并且层数越深影响越大。也可以说当输入的分布发生变化，网络就不得不再去适应新的数据分布。这会导致一个模型非常难训练出来。一旦训练速率与参数初始化不恰当，模型就不会收敛。这
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Detection之RCNN一族</title>
    <link href="http://yoursite.com/2018/04/08/Detection%E4%B9%8BRCNN%E4%B8%80%E6%97%8F/"/>
    <id>http://yoursite.com/2018/04/08/Detection之RCNN一族/</id>
    <published>2018-04-07T16:18:51.000Z</published>
    <updated>2018-04-12T15:10:38.522Z</updated>
    
    <content type="html"><![CDATA[<p>Detection is an important part of computer vision. Today I will take a view at the prevail algorithm in detection areas, named RCNN and it’s extension. </p><h2 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h2><p>RCNN’s meaning is region based CNN. Unlike yolo and yolo9000, RCNN chose the region as the base of it’s predict bounding boxes. Finding that if we just use the location like x, y, then it’s hard to converge, because firsty there maybe exist a couple of objects and then the x, y have difficulty deciding which to approach. secondly, x,y has no limition and can appear in anywhere on the image. So RCNN use prechoosed region and then finetune the bounding boxes, actually compute the scaling and offset. The RCNN’s precess is like this:</p><ul><li>use some methods to generate regions(e.t.c selective search)</li><li>adopt the CNN model to extract the feature.</li><li>put the feature map into the SVM binary classifier.</li><li>put the feature map into the regressor to finetune</li></ul><p>The schemetic diagram is below:<br><img src="/images/Picture1.png" alt=""></p><p>Note that RCNN is puting the every region into the network to train which consume a lot of time. Also note that RCNN had to first wrap resize every region into fixed size because FCN is required fixed-size input. So we will find the image to be a little geometric distortion.</p><p>The training detail is: First we resize the image to 227 * 227, then we pretrain the image on the ILVCR(It has 1000 categories). Next we adopt domain-specific fine-tuning on the PASCAL VOC 2007(21 categories, 20 classes and 1 background). When we test, we put the layer of 4096 dimension into the SVM classifier and likewise we put the layer into the regression to compute the scaling and offset.</p><p>RCNN is the foundation work of image detection using neural network. It still has a lot of drawbacks which will be solved in the next few algorthims.</p><h2 id="SPPnet"><a href="#SPPnet" class="headerlink" title="SPPnet"></a>SPPnet</h2><p>SPPnet add a layer called spatial pyramid pooling layer. The pooling layer is like this:</p><p><img src="/images/Picture5.png" alt=""> </p><p>Suppose we assume that the feature map is H <em> W size. Then we want to put the feature map into 4 </em> 4 so we need to manually regularize the stride to H/4 <em> W/4 and the kernel size of H/4 </em> W/4. Consequnently, the feature map is resized into 4 <em> 4. Likewise, the 2 </em> 2 and 1 * 1 feature map is generated. Then we flat and cluster them into one vector and sent it to the FCN. It plays the role of generate a fix-sized feature map.</p><p>There exist quite a few advantages to use this method. Firstly, RCNN use wrapping to resize the region into fix-size feature map which will cause geometric distortion(like below). And this method will avoid that kind of problem. Moreover, we don’t need to put every region into the network any more. We put the whole image into the network instead. We just need to know where the region is. Depending on this change, the running time is dramatically reduced.(below)</p><p><img src="/images/Picture3.png" alt=""><br><img src="/images/Picture4.png" alt=""></p><h2 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h2><p>We know that RCNN and SPPnet has Multi-stage pipeline —— we need to put the final output into the SVM and then the regressor. Fast RCNN is born to handle the preblem. The trait of Fast RCNN is single-stage and multi-task. The schemetic diagram is below:</p><p><img src="/images/Picture16png.png" alt=""></p><p>They use softmax to replace SVM because they find softmax is litter better than SVM in this task. We find that all tasks is compressed into one network. </p><p>Another change is that they replace the spatial pyramid pooling layer with a ROI pooling layer(ROI : region of interest) which is a simplified version of SPP. They only use one scale like 7 <em> 7 instead of 4 </em> 4 + 2 * 2 + 1 because they experimented to find that complication brings no improvement.</p><h2 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h2><p>The whole growing histroy we can summarize as a process of group all task into one single network. Faster RCNN contributes to moving the generate regions precess to the network where we call it RPN (Region Proposal Network). The overall proceeding is like this:</p><p><img src="/images/Picture7.png" alt=""></p><p>The detailed schemetic diagram is like this:</p><p><img src="/images/Picture8.png" alt=""></p><p>In order to generate appropriate proposals, we need to first define some fix-size and fix-location anchors by ourselves. we need to define different aspect ratio and different size so as to suit different object. The detail is like below:</p><p><img src="/images/Picture9.png" alt=""><br><img src="/images/Picture10.png" alt=""></p><p>We need to explain some place in the picture 2. As we all know, for one feature map and for one pixel, it has a receptive field. If we project a pixel in the feature map somewhere to the raw image. This will be an area which we call grid ceil. For one grid ceil, we define k anchors. In this paper, we consider k as 9 in default. 2k means that for k anchor, there will be two categories, whether it has a object or not. 4k’s 4 means (x, y, w, h) for every anchor.</p><p>Based on the explanation above, we can take a step forward. The detail RPN is like this:</p><p><img src="/images/Picture11.png" alt=""></p><p>The network above is a classification layer. And the third dimension is eighteen filters which is 2 <em> 9(k), 36 is the 4 </em> 9(k). We send all of this into the proposal layer which we include sorting, NMS, and sorting and NMS again(NMS : non maximum supression).</p><h2 id="Mask-RCNN"><a href="#Mask-RCNN" class="headerlink" title="Mask RCNN"></a>Mask RCNN</h2><p>The main change to the Mask RCNN is below:</p><p><img src="/images/Picture12.png" alt=""></p><p>The network add a Lmask task in the final network and replace the ROI pooling with the ROI Align network. The network itself is also improved<br>on the basis of the some methods called ResNeXt-101 and FPN.</p><p>ResNeXt-101 is an updated version of Resnet. The ResNet and ResNeXt-101’s structure is like below:</p><p><img src="/images/Picture13.png" alt=""></p><p>ResNet is a network to solve the draback of deep convolutional network. When the convolutional network is becoming deeper, the accuracy become lower. Because the deeper layer has the responsibility of predict the truth. So we can use the method of boosting tree. For one layer, we only predict the residual of them. For example, we  have the input x, and output is F(x), and the truth is T. If we want to compare the F(x) and T, the pressure is huge. But instead we can use F(x) to compare to the T - x, then it’s easier.  ResNet is a example of this. ResNeXt-101 is an improvement of it. ResNeXt-101 is inspired by the neurons. As we all know, neural network has many neurons which play the role of learn the different feature independetly. So the ResNet-101 use the method of split-transform-merge. The total 32 parts play the different role.</p><p>FPN is born to handle the problem of difficulty of detecting small object. Suppose that a small object is 32 * 32, then if we have a network that has a smaller multiple of 32. Then the last feature map the object only has one pixel. In order to solve the problem, some papers like SSD use the method in the third picture. For the proceeding feature map, each has a predict value. But we know that for every feature map, it has the role of learning different semantics. Like the higher map learn the high semantics and the lower map learn the low semantics. It’s imappropriate to use different feature map to predict the result. So FPN(Feature pyramid network) change it. The lower feature map combine the trait of the higher feature map. But the upsample process has a lot of loss. So we need to use lateral connection —— combine the network before. The detil is as below:<br><img src="/images/Picture14.png" alt=""></p><p>ROI Align solve the issue of misalignment. ROI pooling use the method of neareast neighbor interpolation. Suppose one region is 665 <em> 665, then after some steps of maxpooling, with a division of 32, the pixel is floating point which is impossible. Another point, using ROI pooling, if the size is 20 </em> 20, you want to the fix-size of 7 * 7. Then the stride is also floating point. if we just rouding, the loss will be huge.(See the picture) So ROI Align use the bilinear interpolation.(Picture below). It’s more scientific to use two dimension to rounding. </p><p><img src="/images/Picture15.png" alt=""><br><img src="/images/Picture16.png" alt=""></p><p>We don’t introduce mask, because it’s the area of semanic segmentation.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Detection is an important part of computer vision. Today I will take a view at the prevail algorithm in detection areas, named RCNN and i
      
    
    </summary>
    
      <category term="technology" scheme="http://yoursite.com/categories/technology/"/>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>yolo演变史</title>
    <link href="http://yoursite.com/2018/04/01/yolo%E6%BC%94%E5%8F%98%E5%8F%B2/"/>
    <id>http://yoursite.com/2018/04/01/yolo演变史/</id>
    <published>2018-04-01T12:14:26.000Z</published>
    <updated>2018-04-01T17:59:48.086Z</updated>
    
    <content type="html"><![CDATA[<h2 id="yolo-9000"><a href="#yolo-9000" class="headerlink" title="yolo 9000"></a>yolo 9000</h2><p>yolo suffers from a variety of shortcomings such as high localization loss and low recall compared to region proposal-based methods. So yolo mainly focus on improving recall and localization while maintaining classification accuracy.</p><p>yolo 9000 adopts some methods to better their model like:</p><ul><li><p>Batch Normalization: improve mAP, regularize the model, reduce the overfitting, and make weight initlization easier. 2% mAP</p></li><li><p>High Resolution Classifier: Unlike yolo, we first fine tune the classification network at the full 448*448 resolution and then the detection. </p></li><li><p>Using anchor box:  We know that yolo1 predicts the coordinates of bounding boxes directly using fully connected layers. While Faster RCNN predicts the offsets and confidences for anchor boxes using convolutional layers. So we adopt the anchor boxes method which predicts class and objectness for every anchor box. Note that this method wound be improved below.</p></li><li><p>Unlike Faster RCNN, we don’t use hand-picked priors. This means that we don’t define the width and hight manually but use k-means to learn the priors. We choose k = 5 as a tradeoff between model complexity and high recall. To avoid the imbalance between big boxes and small boxes, we redefine the disance matrix like : d = 1 - IOU(box, centroid). Here’s a code anaysis <a href="https://blog.csdn.net/hrsstudy/article/details/71173305" target="_blank" rel="noopener">k-means yolov2</a></p></li><li><p>Faster RCNN choose (tx, ty) to calculate the (x, y) like: </p><p>  x = (tx * w) - X</p><p>  y = (ty * w) - Y</p><p>  the model is instable because a anchor can end up at any location </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;yolo-9000&quot;&gt;&lt;a href=&quot;#yolo-9000&quot; class=&quot;headerlink&quot; title=&quot;yolo 9000&quot;&gt;&lt;/a&gt;yolo 9000&lt;/h2&gt;&lt;p&gt;yolo suffers from a variety of shortcoming
      
    
    </summary>
    
      <category term="technology" scheme="http://yoursite.com/categories/technology/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>CTC原理及代码解析</title>
    <link href="http://yoursite.com/2018/03/21/CTC%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2018/03/21/CTC原理及代码解析/</id>
    <published>2018-03-21T14:51:21.000Z</published>
    <updated>2018-04-12T15:16:51.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>CTC经常使用于语音识别中，这次我们将基于RNN讲一讲它。</p><p>对语音进行识别的时候，一般需要把语音分成许多时间步，这样的话才能够有办法将其识别，比如：</p><pre><code>_bbee_ee___</code></pre><p>由于语音被拖长，bee这个词只能显示如上，其中还包括了空白(blank)，RNN中，我们把这个词的总时间步(time step)记为T, 而对于每个时间步长，它还有一个维度就是概率分布，假设字母表共有C个字母，则这个词语的维度就是C * T。</p><p>上面的例子是我们记录下来的形式，实际真实的标签(ground truth)应该是bee，肯定小于记录形式的长度。在这里，我需要对真实标签进行扩充。对于bee，我需要如下的表示:</p><pre><code>_b_e_e_</code></pre><p>对于这样一个中间的表示，我需要设它的长度为S，其原来真实标签的长度为L, 故有以下关系，S = 2L + 1。</p><p>接下来我们需要做的是前向计算（forward algorithm），其中还提到了动态规划（Dynamic Programing），了解了一下，不过比较迷，还未找到之间的联系。</p><p>这里我们假设对于RNN来说：其输出结果是[R1, R2, R3, R4], 而对于真实标签的中间状态G为[blank, g1, blank, g2, blank, g3, blank]。我们知道，R1只能对应blank或者g1, 而因此对于 t = 1, 我们有概率[P[0][1], P[1][1], 0, 0, 0, 0, 0]，对于t = 2, 我们有[0, (P[0][1] + P[1][1]) <em> P[1][2], P[1][1] </em> P[2][2], P[1][1] * P[3][2], 0, 0, 0], 以此类推。这实际上就是一种条件概率，根据前面的时间步的概率来计算下一个时间步的概率。从这一点来看，的确是借鉴了动态规划的思想就是将复杂的问题转化为一个个简单的子问题。</p><h1 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h1><p>其实根据前面的概率分析我们知道，输出结果对应真实标签的输出状态的概率分布是有一定规律的，从某个时刻开始某一项之前就不能再有对应项了，不然的话，输出就无法走不完全程，也就是无法对应全部真实标签，同理，在开始的时候，前几个输出项也不能对应G的后面几项。因此，我们设置start 和 end两个标准，所有对应只能在它们之间。下面是start 和 end 的计算方式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int remain = (S / 2) + repeats - (T - t)</span><br><span class="line">if(remain &gt; 0)&#123;</span><br><span class="line">    start += s_inc[remain];</span><br><span class="line">&#125;</span><br><span class="line">if(t &lt;= (S / 2) + repeats)&#123;</span><br><span class="line">    end += e_inc[t - 1]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们先不看repeats, S / 2是真实标签的数量, 当 t = 0 的时候，remain应该是负数，当remain&gt;0的时候，意味着start不能待在开始，n = 1，意味着start需要到2处，n = 3，说明start需要到6处，以此类推，所以在不考虑repeats的情况下，每次start加2，而end同理。</p><p>但是我们必须考虑有repeats的情况，当真实标签中有重复比如bee的情况的时候，两个e之间的空白不能跳过，不然我们会认为只有一个e,因此start和end在遇到有重复的情况的时候，只能加一，加一这种状态必须出现两次。repeats每多一次，starts的增加就少一次，所以remai必须加上repeats以保证提前变为正数。由此，以上代码就解释的通了。</p><p>通过以上解释，数组的初始化也应该说得通了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">nt e_counter = 0;</span><br><span class="line">int s_counter = 0;</span><br><span class="line"></span><br><span class="line">s_inc[s_counter++] = 1;</span><br><span class="line"></span><br><span class="line">int repeats = 0;</span><br><span class="line"></span><br><span class="line">for (int i = 1; i &lt; L; ++i) &#123;</span><br><span class="line">    if (labels[i-1] == labels[i]) &#123;</span><br><span class="line">        s_inc[s_counter++] = 1;</span><br><span class="line">        s_inc[s_counter++] = 1;</span><br><span class="line">        e_inc[e_counter++] = 1;</span><br><span class="line">        e_inc[e_counter++] = 1;</span><br><span class="line">        ++repeats;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        s_inc[s_counter++] = 2;</span><br><span class="line">        e_inc[e_counter++] = 2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">e_inc[e_counter++] = 1;</span><br></pre></td></tr></table></figure></p><h1 id="更新-2018-3-24"><a href="#更新-2018-3-24" class="headerlink" title="更新 2018.3.24"></a>更新 2018.3.24</h1><p>之前的是对CTC原理中动态规划部分的主要解释，今天我们来讲一讲它就是如何应用到RNN中去的。为了方便，我将采用CRNN模型，<a href="https://arxiv.org/abs/1507.05717" target="_blank" rel="noopener">论文地址</a>。</p><p>了解CRNN后知道，CRNN的输出是一个序列y = y1, y2, y3, … yT, 其中yt是一个字母表+空白的概率分布，我们定义π是y的其中一种情况(path)，定义一个函数(Sequqnce to Sequence)B，B(π)是产生一个标签(label)，比如B(–he–l-l–oo-)是hello，我们假设真实标签是l， 那么在训练的时候，我们首先需要求出输入在进入CTC之后，所产生的输出是l的概率，也就是条件概率。如图所示:</p><p><img src="/images/math1.png" alt=""></p><p>根据这个概率，我们可以采用negative log-likelihood来计算损失函数，从而bp和optimize。这里面，p(π|y) = Π(yt(πt)),如果直接计算p(π|y)当然非常复杂，所以CTC采用了动态规划的思想，也就是我上面介绍的内容。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;CTC经常使用于语音识别中，这次我们将基于RNN讲一讲它。&lt;/p&gt;
&lt;p&gt;对语音进行识别的时候，一般需要把语音分成许多时间步，这样的话才能够
      
    
    </summary>
    
      <category term="technology" scheme="http://yoursite.com/categories/technology/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>给自己的规矩</title>
    <link href="http://yoursite.com/2018/03/19/%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84%E8%A7%84%E7%9F%A9/"/>
    <id>http://yoursite.com/2018/03/19/给自己的规矩/</id>
    <published>2018-03-18T20:18:52.000Z</published>
    <updated>2018-03-18T20:28:19.412Z</updated>
    
    <content type="html"><![CDATA[<p>根据之前自己的文章《最近的想法》而制定的自己的一些规矩，希望自己能够一直坚持下去：</p><ul><li><p>手机不允许带到床上</p></li><li><p>晚上没课去图书馆，回来去七楼，十一点下去学英语和一些杂事。</p></li><li><p>每天写daily report, 每周写weekly report</p></li><li><p>一周一篇论文，需要完全读懂。</p></li><li><p>十二点一定要上床。</p></li><li><p>不允许带耳机出门。</p></li></ul><p>写下来以便时刻提醒监督自己。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;根据之前自己的文章《最近的想法》而制定的自己的一些规矩，希望自己能够一直坚持下去：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;手机不允许带到床上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;晚上没课去图书馆，回来去七楼，十一点下去学英语和一些杂事。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;每天写da
      
    
    </summary>
    
      <category term="life" scheme="http://yoursite.com/categories/life/"/>
    
    
      <category term="日记" scheme="http://yoursite.com/tags/%E6%97%A5%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Evaluation Standard</title>
    <link href="http://yoursite.com/2018/03/10/Evaluation-Standard/"/>
    <id>http://yoursite.com/2018/03/10/Evaluation-Standard/</id>
    <published>2018-03-10T11:01:50.000Z</published>
    <updated>2018-04-11T18:52:27.696Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluation-Standard"><a href="#Evaluation-Standard" class="headerlink" title="Evaluation Standard"></a>Evaluation Standard</h1><h3 id="Accurency-Precision-Recall-F1-ROC-AUC"><a href="#Accurency-Precision-Recall-F1-ROC-AUC" class="headerlink" title="Accurency/Precision/Recall/F1/ROC/AUC"></a>Accurency/Precision/Recall/F1/ROC/AUC</h3><h2 id="Accurency"><a href="#Accurency" class="headerlink" title="Accurency"></a>Accurency</h2><p>accurency = right / all (right: the instance you predict right, all； all sample)</p><h2 id="Precision-and-Recall-and-F1"><a href="#Precision-and-Recall-and-F1" class="headerlink" title="Precision and Recall and F1"></a>Precision and Recall and F1</h2><h3 id="1-confusion-matrix"><a href="#1-confusion-matrix" class="headerlink" title="1. confusion matrix"></a>1. confusion matrix</h3><p>TP: true positive   FN: false negative</p><p>FP: false positive   TP: True negative</p><h3 id="2-Precision"><a href="#2-Precision" class="headerlink" title="2. Precision"></a>2. Precision</h3><p>P = TP/(TP + FP)</p><p>It refers to the accurency rate of you prediction</p><h3 id="3-Recall"><a href="#3-Recall" class="headerlink" title="3. Recall"></a>3. Recall</h3><p>R = TP/(TP + FN)</p><p>It refers to the comprehensive rate of your prediction</p><h3 id="4-Precision-and-Recall’s-description"><a href="#4-Precision-and-Recall’s-description" class="headerlink" title="4. Precision and Recall’s description"></a>4. Precision and Recall’s description</h3><p>The Precision and Recall are often in conflict with each other, so if one P-R curve is completely wrapped by another, then we can assume that the latter is better.</p><h3 id="5-F1"><a href="#5-F1" class="headerlink" title="5. F1"></a>5. F1</h3><p>F1 is the harmonic mean of the precision and recall</p><p>definition: F1 = (2 <em> P </em> R)/(P + R)</p><p>If we want to give a weight to the each evaluation method, then we can use the β like this</p><p>F1 = (1 + β^2) <em> P </em> R/((β^2 * P) + B)</p><h3 id="6-extension-and-application"><a href="#6-extension-and-application" class="headerlink" title="6. extension and application"></a>6. extension and application</h3><p>Sometimes we some other situation:</p><ul><li>have trained and tested quite a few times and we get severial confusion matrix  </li><li>have trained and tested in different datasets, hoping to evaluate the algorithm as a whole</li><li>for Multi-category tasks, we combine each two category into a confusion matrix</li></ul><p>one method:</p><p>calculate each Precision and Recall in different confusion matrix and then calculate the mean of them(called macro-P, macro-R and macro-F1)</p><p>macro-P = 1/n <em> ∑P<br>macro-R = 1/n </em> ∑R<br>macro-F1 = (2 <em> macro-P </em> macro-R)/(macro-P + macro-R)</p><p>another method:</p><p>calculate the mean of TP, FP, FN, TN, then calculate the micro-P, micro-R and micro-F1</p><h2 id="ROC-and-AUC"><a href="#ROC-and-AUC" class="headerlink" title="ROC and AUC"></a>ROC and AUC</h2><p>So many learning model generate a value or probablity prediction and then compare it with the threshold. Then we cansort the value and get a cut point and divide the sample into two part. So the quality of sorting represent the ability of generalization, anfd we can use the ROC curve</p><p><img src="/images/ROC.png" alt=""></p><p>If we assume that one marker is (x, y), then if the next sample is the true positive, then the coordinate of it is (x, y + 1/m+), and if it is false positive, then the coordinate of it is (x + 1/m-, y)</p><p>Just like above, if one learning model’s ROC curve is wrapped by the another, then the latter is better. Another method to measure is to compare the <em>Areas Under the ROC Curve</em>(<strong>AUC</strong>)</p><p>defintion: AUC = 1/2 ∑(xi+1 - xi) <em> (yi + yi+1)   </em>i from 1 to m-1*</p><p>AUC is the evaluation of the quality of prediction sorting, if we have m+ positive example and m- negative example, and D+ and D- represent positive and negative set, then we can define loss as:</p><p>lrank = 1/m+ <em> m- ∑ ∑ (II(f(x+) &lt; f(x-)) + 1/2 </em> II(f(x+) = f(x-)))</p><p>This means that if the positive prediction value is smaller than the negative, then you are punished by one point, the same as another. We can find that AUC = 1 - lrank</p><h2 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h2><p>mAP is often used in computer vision which means “mean average precision”. Suppose that we have ten true objects and then we can have ten recall objects, for every recall target, we choose the maximum precision among the index of target and later target as the precision of this recall target. Then we calculate the average of them as the average precision(<strong>AP</strong>), that’s for two categories, if we have multiple categories. then for every category and other categories viewed as one category, we also adopt this method and calculate the means(<strong>mean average precision</strong>).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Evaluation-Standard&quot;&gt;&lt;a href=&quot;#Evaluation-Standard&quot; class=&quot;headerlink&quot; title=&quot;Evaluation Standard&quot;&gt;&lt;/a&gt;Evaluation Standard&lt;/h1&gt;&lt;h3 i
      
    
    </summary>
    
      <category term="technology" scheme="http://yoursite.com/categories/technology/"/>
    
    
      <category term="Machine learning" scheme="http://yoursite.com/tags/Machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>读资本论</title>
    <link href="http://yoursite.com/2018/03/06/%E8%AF%BB%E8%B5%84%E6%9C%AC%E8%AE%BA/"/>
    <id>http://yoursite.com/2018/03/06/读资本论/</id>
    <published>2018-03-05T23:15:31.000Z</published>
    <updated>2018-03-05T23:18:26.603Z</updated>
    
    <content type="html"><![CDATA[<p>百忙之中偶有余闲，决定读一读《资本论》，权当兴趣使然。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;百忙之中偶有余闲，决定读一读《资本论》，权当兴趣使然。&lt;/p&gt;

      
    
    </summary>
    
      <category term="life" scheme="http://yoursite.com/categories/life/"/>
    
    
      <category term="经济" scheme="http://yoursite.com/tags/%E7%BB%8F%E6%B5%8E/"/>
    
  </entry>
  
  <entry>
    <title>Faster RCNN 详解与实现</title>
    <link href="http://yoursite.com/2018/02/26/Faster-RCNN-%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2018/02/26/Faster-RCNN-详解与实现/</id>
    <published>2018-02-25T16:54:50.000Z</published>
    <updated>2018-03-05T23:09:48.029Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在研究CV领域的一些算法，由于本人编程水平较弱加之faster rcnn算法比较复杂，虽然原理在很早就大致看懂，但在看源码的时候仍然吃了不少苦头，近些时候，终于了解了代码的大致含义并借鉴大神们的代码自己用pytorch实现了一下。在此，想跟大家详细地解释一下代码的含义， 帮助读者更好的理解这一算法。本人代码如下：</p><p><a href="https://github.com/starcraft2chunjie/Faster-RCNN-pytorch/tree/master" target="_blank" rel="noopener">Faster-RCNN-pytorch</a></p><p>需要先了解一下faster rcnn的读者可以点击以下链接，这是本人看到的比较全面细致的faster rcnn教程。</p><p><a href="https://www.jianshu.com/p/de37451a0a77?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation" target="_blank" rel="noopener">Faster RCNN 详解</a></p><h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><p>这一部分对应model_easy.py，我们知道, Faster RCNN 总共实际上就是四个模型，CNN模型， RPN(Region proposal layer), ROI Pooling layer, Classification layer(可以称为Faster RCNN层)。CNN层使用预训练的vgg16提取特征，得到feature map，然后进入RPN层, RPN层s首先经过一个kernel_size为3的层,之后再分为两部分，分别输出2<em>9大小和4</em>9大小的向量（对应于9 anchor <em> 2 classifier以及 9 anchor </em> 4 coordinate）,通过训练修正proposal的位置以及数量（通过rpn文件里面的proposal_layer.py），修正好的proposal进入ROI Pooling层统一划成7*7大小，最后进入FasterRCNN层输出最后的bbox_pred(bbox_delta)以及scores。</p><p>CNN层比较简单，在这里不再赘述。我们看一看RPN层的源码。</p><h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># the simple model of RPN</span><br><span class="line">class RPN(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(RPN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=(1, 1)),</span><br><span class="line">                                            nn.ReLU())</span><br><span class="line"></span><br><span class="line">        # 9 anchor * 2 classfier (object or non-object) each grid</span><br><span class="line">        self.conv1 = nn.Conv2d(512, 2 * 9, kernel_size=1, stride=1)</span><br><span class="line"></span><br><span class="line">        # 9 anchor * 4 coordinate regressor each grids</span><br><span class="line">        self.conv2 = nn.Conv2d(512, 4 * 9, kernel_size=1, stride=1)</span><br><span class="line">        self.softmax = nn.Softmax()</span><br><span class="line"></span><br><span class="line">    def forward(self, features):</span><br><span class="line"></span><br><span class="line">        features = self.conv(features)</span><br><span class="line"></span><br><span class="line">        logits, rpn_bbox_pred = self.conv1(features), self.conv2(features)</span><br><span class="line"></span><br><span class="line">        height, width = features.size()[-2:]</span><br><span class="line">        logits = logits.squeeze(0).permute(1, 2, 0).contiguous()  # (1, 18, H/16, W/16) =&gt; (H/16 ,W/16, 18)</span><br><span class="line">        logits = logits.view(-1, 2)  # (H/16 ,W/16, 18) =&gt; (H/16 * W/16 * 9, 2)</span><br><span class="line"></span><br><span class="line">        rpn_cls_prob = self.softmax(logits)</span><br><span class="line">        rpn_cls_prob = rpn_cls_prob.view(height, width, 18)  # (H/16 * W/16 * 9, 2)  =&gt; (H/16 ,W/16, 18)</span><br><span class="line">        rpn_cls_prob = rpn_cls_prob.permute(2, 0, 1).contiguous().unsqueeze(0) # (H/16 ,W/16, 18) =&gt; (1, 18, H/16, W/16)</span><br><span class="line"></span><br><span class="line">        return rpn_bbox_pred, rpn_cls_prob, logits</span><br></pre></td></tr></table></figure><p>注意forward的返回值，一个是rpn_bbox_pred，这个是我们我提到的教程里面的预测的[dx, dy, dh, dw]，也是之后的bbox_delta。rpn_cls_prob也仅仅只是logits经过了一层softmax函数而已。模型大致架构比较简单，下面来看看与RPN层相关的函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">def proposal(self, rpn_bbox_pred, rpn_cls_prob, im_info, test, args):</span><br><span class="line">       &quot;&quot;&quot;</span><br><span class="line">       Arguments:</span><br><span class="line">           rpn_bbox_pred (Tensor) : (1, 4*9, H/16, W/16)</span><br><span class="line">           rpn_cls_prob (Tensor) : (1, 2*9, H/16, W/16)</span><br><span class="line">           im_info (Tuple) : (Height, Width, Channel, scale_ratios)</span><br><span class="line">           test (Bool) : True or False</span><br><span class="line">           args (argparse.Namespace) : global arguments</span><br><span class="line">       Return:</span><br><span class="line">           # in each minibatch number of proposal boxes is variable</span><br><span class="line">           proposals_boxes (Ndarray) : ( # proposal boxes, 4)</span><br><span class="line">           scores (Ndarray) :  ( # proposal boxes, )</span><br><span class="line">       &quot;&quot;&quot;</span><br><span class="line">       &quot;&quot;&quot;</span><br><span class="line">       # Algorithm:</span><br><span class="line">       #</span><br><span class="line">       # for each (H, W) location i</span><br><span class="line">       #   generate A anchor boxes centered on cell i</span><br><span class="line">       #   apply predicted bbox deltas at cell i to each of the A anchors</span><br><span class="line">       # clip predicted boxes to image</span><br><span class="line">       # remove predicted boxes with either height or width &lt; threshold</span><br><span class="line">       # sort all (proposal, score) pairs by score from highest to lowest</span><br><span class="line">       # take top pre_nms_topN proposals before NMS</span><br><span class="line">       # apply NMS with threshold 0.7 to remaining proposals</span><br><span class="line">       # take after_nms_topN proposals after NMS</span><br><span class="line">       # return the top proposals (-&gt; RoIs top, scores top)</span><br><span class="line">       #layer_params = yaml.load(self.param_str_)</span><br><span class="line">       &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       anchors = generate_anchors()</span><br><span class="line">       _num_anchors = anchors.shape[0]</span><br><span class="line"></span><br><span class="line">       all_anchors = get_anchor(rpn_cls_prob, anchors)   # [H * W * 9, 4]</span><br><span class="line"></span><br><span class="line">       pre_nms_topn = args.pre_nms_topn if test == False else args.test_pre_nms_topn</span><br><span class="line">       nms_thresh = args.nms_thresh if test == False else args.test_nms_thresh</span><br><span class="line">       post_nms_topn = args.post_nms_topn if test == False else args.test_post_nms_topn</span><br><span class="line"></span><br><span class="line">       &quot;&quot;&quot;It&apos;s directly from anchor_target_layer, essentially from training the RPN&quot;&quot;&quot;</span><br><span class="line">       bbox_deltas = self._get_bbox_deltas(rpn_bbox_pred).data.cpu().numpy()</span><br><span class="line"></span><br><span class="line">       # 1. Convert anchors into proposal via bbox transformation</span><br><span class="line">       &quot;&quot;&quot;Here we need to generate the precise proposal location for the later operation&quot;&quot;&quot;</span><br><span class="line">       proposals_boxes = bbox_transform_inv(all_anchors, bbox_deltas)  # (H/16 * W/16 * 9, 4) all proposal boxes</span><br><span class="line">       scores = self._get_pos_score(rpn_cls_prob).data.cpu().numpy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       # 2. clip predicted boxes to image</span><br><span class="line">       proposals = clip_boxes(proposals_boxes, im_info[:2])</span><br><span class="line"></span><br><span class="line">        # 3. remove predicted boxes with either height or width &lt; threshold</span><br><span class="line">       # (NOTE: convert min_size to input image scale stored in im_info[3])</span><br><span class="line">       keep = filter_boxes(proposals_boxes, self.args.min_size * max(im_info[3]))</span><br><span class="line">       proposals = proposals[keep, :]</span><br><span class="line">       scores = scores[keep]</span><br><span class="line"></span><br><span class="line">       # 4. sort all (proposal, score) pairs by score from highest to lowest</span><br><span class="line">       # 5. take top pre_nms_topn (e.g. 6000)</span><br><span class="line">       order = scores.ravel().argsort()[::-1]</span><br><span class="line">       if pre_nms_topn &gt; 0:</span><br><span class="line">           order = order[:pre_nms_topn]</span><br><span class="line">       proposals = proposals[order, :]</span><br><span class="line">       scores = scores[order]</span><br><span class="line"></span><br><span class="line">       # 6. apply nms (e.g. threshold = 0.7)</span><br><span class="line"></span><br><span class="line">       keep = py_cpu_nms(np.hstack((proposals, scores)), nms_thresh)</span><br><span class="line"></span><br><span class="line">       # 7. take after_nms_topN (e.g. 300)</span><br><span class="line">       if post_nms_topn &gt; 0:</span><br><span class="line">           keep = keep[:post_nms_topn]</span><br><span class="line">       </span><br><span class="line">       # 8. return the top proposals (-&gt; RoIs top)</span><br><span class="line">       proposals = proposals[keep, :]</span><br><span class="line">       scores = scores[keep]</span><br><span class="line">       batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)</span><br><span class="line">       blob = np.hstack((batch_inds, proposals.astype(np.float32, copy = False)))</span><br><span class="line">       return blob</span><br></pre></td></tr></table></figure><p>这段代码的作用是什么？用教程里面的话就是：</p><blockquote><ol><li>生成anchors，利用[dx(A)，dy(A)，dw(A)，dh(A)]对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）</li></ol></blockquote><blockquote><ol><li>按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。</li></ol></blockquote><blockquote><ol><li>利用im_info将fg anchors从MxN尺度映射回PxQ原图，判断fg anchors是否大范围超过边界，剔除严重超出边界fg anchors。</li></ol></blockquote><blockquote><ol><li>进行nms（nonmaximum suppression，非极大值抑制）</li></ol></blockquote><blockquote><ol><li>再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。</li></ol></blockquote><blockquote><ol><li>之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的</li></ol></blockquote><p>简单来说，就是对产生的所有anchor进行修正以及筛选。此函数输入的主要参数是之前RPN网络输出的rpn_bbox_pred以及rpn_cls_prob。这里有几行代码需要解释一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bbox_deltas = self._get_bbox_deltas(rpn_bbox_pred).data.cpu().numpy()</span><br><span class="line"></span><br><span class="line">proposals_boxes = bbox_transform_inv(all_anchors, bbox_deltas)  # (H/16 * W/16 * 9, 4) all proposal boxes</span><br><span class="line"></span><br><span class="line">scores = self._get_pos_score(rpn_cls_prob).data.cpu().numpy()</span><br></pre></td></tr></table></figure><p>_get_bbox_deltas仅仅是把rpn_bbox_pred转换为了（H/16 <em> W/16 </em> 9, 4）, bbox_transform_inv则是根据bbox_deltas以及anchors生成了对应的pred_anchor(注意之前的rpn_bbox_pred仅仅是生成的[dx, dy, dh, dw]，不是最终确定的proposal)，_get_pos_score也仅仅是将rpn_cls_score转化为(H/16 <em> W/16 </em> 9, 1)</p><h3 id="ROI-Pooling"><a href="#ROI-Pooling" class="headerlink" title="ROI Pooling"></a>ROI Pooling</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class ROIpooling(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, size=(7, 7), spatial_scale=1.0 / 16.0):</span><br><span class="line">        super(ROIpooling, self).__init__()</span><br><span class="line">        self.adapmax2d = nn.AdaptiveMaxPool2d(size)</span><br><span class="line">        self.spatial_scale = spatial_scale</span><br><span class="line"></span><br><span class="line">    def forward(self, features, rois_boxes):</span><br><span class="line"></span><br><span class="line">        # rois_boxes : [x, y, x`, y`]</span><br><span class="line"></span><br><span class="line">        if type(rois_boxes) == np.ndarray:</span><br><span class="line">            rois_boxes = to_var(torch.from_numpy(rois_boxes))</span><br><span class="line"></span><br><span class="line">        rois_boxes = rois_boxes.data.float().clone()</span><br><span class="line">        rois_boxes.mul_(self.spatial_scale)</span><br><span class="line">        rois_boxes = rois_boxes.long()</span><br><span class="line"></span><br><span class="line">        output = []</span><br><span class="line"></span><br><span class="line">        for i in range(rois_boxes.size(0)):</span><br><span class="line">            roi = rois_boxes[i]</span><br><span class="line"></span><br><span class="line">            try:</span><br><span class="line"></span><br><span class="line">                roi_feature = features[:, :, roi[1]:(roi[3] + 1), roi[0]:(roi[2] + 1)]</span><br><span class="line">            except Exception as e:</span><br><span class="line">                print(e, roi)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            pool_feature = self.adapmax2d(roi_feature)</span><br><span class="line">            output.append(pool_feature)</span><br><span class="line"></span><br><span class="line">        return torch.cat(output, 0)</span><br></pre></td></tr></table></figure><p>ROI Pooling显得比较简单，但这并不是因为其算法本身简单，实际上，此层需要实现的算法是对任意输入的proposal，都要加工成7*7大小，这个工作量似乎不小，实际上，之前大神的代码里，ROI Pooling的实现代码很长，但正因为如此，pytorch的contributor将其封装，现在仅仅只需要调用adapmax2d就可以了。</p><h3 id="FasterRCNN"><a href="#FasterRCNN" class="headerlink" title="FasterRCNN"></a>FasterRCNN</h3><p>这个代码比较简单，没什么好讲的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class FasterRcnn(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(FasterRcnn, self).__init__()</span><br><span class="line">        self.fc1 = nn.Sequential(nn.Linear(512 * 7 * 7, 4096),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout())</span><br><span class="line"></span><br><span class="line">        self.fc2 = nn.Sequential(nn.Linear(4096, 4096),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout())</span><br><span class="line"></span><br><span class="line">        # 20 class + 1 backround classfier each roi</span><br><span class="line">        self.classfier = nn.Linear(4096, 21)</span><br><span class="line">        self.softmax = nn.Softmax()</span><br><span class="line"></span><br><span class="line">        # 21 class * 4 coordinate regressor each roi</span><br><span class="line">        self.regressor = nn.Linear(4096, 21 * 4)</span><br><span class="line"></span><br><span class="line">    def forward(self, features):</span><br><span class="line"></span><br><span class="line">        features = features.view(-1, 512 * 7 * 7)</span><br><span class="line">        features = self.fc1(features)</span><br><span class="line">        features = self.fc2(features)</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            logits = self.classfier(features)</span><br><span class="line">            scores = self.softmax(logits)</span><br><span class="line">            bbox_delta = self.regressor(features)</span><br><span class="line"></span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(e, logits)</span><br><span class="line"></span><br><span class="line">        return bbox_delta, scores, logits</span><br></pre></td></tr></table></figure><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>loss部分主要分为两个部分，rpn_loss以及fasterRCNN_loss</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">def rpn_loss(rpn_cls_prob, rpn_logits, rpn_bbox_pred, rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights):</span><br><span class="line">    </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">        rpn_cls_prob (Tensor): (1, 2*9, H/16, W/16)</span><br><span class="line">        rpn_logits (Tensor): (H/16 * W/16 * 9 , 2) object or non-object rpn_logits</span><br><span class="line">        rpn_bbox_pred (Tensor): (1, 4*9, H/16, W/16) predicted boxes</span><br><span class="line">        rpn_labels (Ndarray) : (H/16 * W/16 * 9 ,)</span><br><span class="line">        rpn_bbox_targets (Ndarray) : (H/16 * W/16 * 9, 4)</span><br><span class="line">        rpn_bbox_inside_weights (Ndarray) : (H/16 * W/16 * 9, 4) masking for only positive box loss</span><br><span class="line">    Return:</span><br><span class="line">        cls_loss (Scalar) : classfication loss</span><br><span class="line">        reg_loss * 10 (Scalar) : regression loss</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    height, width = rpn_cls_prob.size()[-2:]  # (H/16, W/16)</span><br><span class="line">    rpn_cls_prob = rpn_cls_prob.squeeze(0).permute(1, 2, 0).contiguous()  # (1, 18, H/16, W/16) =&gt; (H/16 ,W/16, 18)</span><br><span class="line">    rpn_cls_prob = rpn_cls_prob.view(-1, 2)  # (H/16 ,W/16, 18) =&gt; (H/16 * W/16 * 9, 2)</span><br><span class="line"></span><br><span class="line">    rpn_labels = to_tensor(rpn_labels).long() # convert properly # (H/16 * W/16 * 9)</span><br><span class="line"></span><br><span class="line">    #index where not -1</span><br><span class="line">    idx = rpn_labels.ge(0).nonzero()[:, 0]</span><br><span class="line">    rpn_cls_prob = rpn_cls_prob.index_select(0, to_var(idx))</span><br><span class="line">    rpn_labels = rpn_labels.index_select(0, idx)</span><br><span class="line">    rpn_logits = rpn_logits.squeeze().index_select(0, to_var(idx))</span><br><span class="line"></span><br><span class="line">    positive_cnt = torch.sum(rpn_labels.eq(1))</span><br><span class="line">    negative_cnt = torch.sum(rpn_labels.eq(0))</span><br><span class="line"></span><br><span class="line">    rpn_labels = to_var(rpn_labels)</span><br><span class="line"></span><br><span class="line">    cls_crit = nn.CrossEntropyLoss()</span><br><span class="line">    cls_loss = cls_crit(rpn_logits, rpn_labels)</span><br><span class="line"></span><br><span class="line">    rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets)</span><br><span class="line">    rpn_bbox_targets = rpn_bbox_targets.view(height, width, 36)  # (H/16 * W/16 * 9, 4)  =&gt; (H/16 ,W/16, 36)</span><br><span class="line">    rpn_bbox_targets = rpn_bbox_targets.permute(2, 0, 1).contiguous().unsqueeze(0) # (H/16 ,W/16, 36) =&gt; (1, 36, H/16, W/16)</span><br><span class="line">    rpn_bbox_targets = to_var(rpn_bbox_targets)</span><br><span class="line"></span><br><span class="line">    rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights)</span><br><span class="line">    rpn_bbox_inside_weights = rpn_bbox_inside_weights.view(height, width, 36)  # (H/16 * W/16 * 9, 4)  =&gt; (H/16 ,W/16, 36)</span><br><span class="line">    rpn_bbox_inside_weights = rpn_bbox_inside_weights.permute(2, 0, 1).contiguous().unsqueeze(0) # (H/16 ,W/16, 36) =&gt; (1, 36, H/16, W/16)</span><br><span class="line"></span><br><span class="line">    rpn_bbox_inside_weights = rpn_bbox_inside_weights.cuda() if torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">    rpn_bbox_pred = to_var(torch.mul(rpn_bbox_pred.data, rpn_bbox_inside_weights))</span><br><span class="line">    rpn_bbox_targets = to_var(torch.mul(rpn_bbox_targets.data, rpn_bbox_inside_weights))</span><br><span class="line"></span><br><span class="line">    reg_loss = F.smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, size_average = False) / (positive_cnt + 1e-4)</span><br><span class="line"></span><br><span class="line">    return cls_loss, reg_loss * 10</span><br></pre></td></tr></table></figure><p>首先注意一点，rpn_bbox_targets, rpn_bbox_inside_weights这两个参数是从anchor_target此函数得来的。</p><p>rpn_loss的主要操作流程如下：</p><ul><li><p>筛选出label值不是-1的proposal(-1表示don’t care area)</p></li><li><p>计算出是前景的proposal和是背景的proposal的数目。</p></li><li><p>classification使用CrossEntropyLoss(注意CrossEntropyLoss已经包含了log softmaxLoss,所以只需要使用logits作为参数)</p></li><li><p>regression使用smooth_l1_loss</p></li><li><p>综合两个函数</p></li></ul><p>frcnn_loss与之相近，就不再赘述</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近一直在研究CV领域的一些算法，由于本人编程水平较弱加之faster rcnn算法比较复杂，虽然原理在很早就大致看懂，但在看源码的时候仍然吃了不少苦头，近些时候，终于了解了代码的大致含义并借鉴大神们的代码自己用pytorch实现了一下。在此，想跟大家详细地解释一下代码的含
      
    
    </summary>
    
      <category term="technology" scheme="http://yoursite.com/categories/technology/"/>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
</feed>
